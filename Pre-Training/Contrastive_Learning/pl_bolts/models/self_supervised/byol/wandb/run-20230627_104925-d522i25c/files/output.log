
Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s]
GPU available: True, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/wolfda/anaconda3/envs/swavlightning_next/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1823: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.
  rank_zero_warn(
/home/wolfda/anaconda3/envs/swavlightning_next/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:133: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
  rank_zero_warn("You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.")
/home/wolfda/anaconda3/envs/swavlightning_next/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:326: LightningDeprecationWarning: Base `LightningModule.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
  rank_zero_deprecation(
/home/wolfda/anaconda3/envs/swavlightning_next/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /home/wolfda/Data/Spark/PreTrain/BYOL exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
  | Name           | Type       | Params
----------------------------------------------
0 | online_network | SiameseArm | 35.1 M
1 | target_network | SiameseArm | 35.1 M
----------------------------------------------
70.1 M    Trainable params
0         Non-trainable params
70.1 M    Total params
280.474   Total estimated model params size (MB)
/home/wolfda/anaconda3/envs/swavlightning_next/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1938: PossibleUserWarning: The number of training samples (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s]torch.Size([64, 3, 224, 224])
torch.Size([64, 3, 224, 224])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
tensor([[0.8254, 0.8350, 0.8330,  ..., 1.2050, 0.7217, 0.5666],
        [0.8261, 0.9092, 0.9471,  ..., 1.6255, 0.7557, 0.4704],
        [0.7173, 0.8350, 0.6868,  ..., 1.0418, 0.9761, 0.5403],
        ...,
        [0.8999, 0.7970, 0.9517,  ..., 1.0444, 0.7018, 0.7375],
        [0.5508, 0.7542, 0.9858,  ..., 0.9607, 0.7728, 0.4584],
        [1.0617, 0.8379, 0.7163,  ..., 1.2428, 1.0058, 0.6173]],
       grad_fn=<ReshapeAliasBackward0>)
tensor([[-0.1241,  0.0467,  0.1133,  ...,  0.2311, -0.2589, -0.0281],
        [ 0.0930, -0.2114, -0.0206,  ...,  0.2466, -0.3783, -0.0507],
        [ 0.0171, -0.1638, -0.0646,  ...,  0.1536, -0.3497,  0.0977],
        ...,
        [ 0.1228, -0.2599,  0.1055,  ...,  0.1800, -0.5569, -0.0836],
        [-0.0581,  0.1138,  0.0228,  ..., -0.1654, -0.6232, -0.0182],
        [ 0.0359, -0.2602,  0.0917,  ...,  0.2271, -0.4779,  0.0667]],
       grad_fn=<AddmmBackward0>)
tensor([[-0.0758,  0.2660, -0.1106,  ..., -0.1123,  0.0751,  0.1722],
        [ 0.3977,  0.0636,  0.1136,  ..., -0.1063,  0.0351, -0.1312],
        [ 0.2465, -0.0900,  0.1202,  ..., -0.3475,  0.1632,  0.1262],
        ...,
        [-0.2334,  0.1687, -0.3263,  ...,  0.2177, -0.0533, -0.1675],
        [-0.0456, -0.0404, -0.1151,  ..., -0.0796,  0.2392,  0.1558],
        [ 0.1903, -0.0374, -0.1429,  ..., -0.1708, -0.2362, -0.0324]],
       grad_fn=<AddmmBackward0>)
/home/wolfda/anaconda3/envs/swavlightning_next/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:727: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
